{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35lDGCTgmf0R"
      },
      "source": [
        "# **‚ú¶Ô∏é Fine-Tuning MetaCLIP-2 for Image Classification**\n",
        "\n",
        "This Colab notebook demonstrates how to fine-tune [MetaCLIP-2](https://huggingface.co/facebook/metaclip-2-worldwide-s16), a powerful multilingual vision-language encoder, for downstream image classification tasks. < style=\"background-color: blue;\"> [Meta CLIP 2](https://huggingface.co/collections/merve/metaclip2-multilingual) is a new vision-language encoder and a training recipe developed by Meta AI, designed to train CLIP models from scratch using a vast, worldwide dataset of image-text pairs. It solves the \"curse of multilinguality\" where adding non-English data often harms English performance by developing a method for jointly scaling data curation, model capacity, and training to ensure both English and non-English data are mutually beneficial. This leads to improved performance on both multilingual and English-only benchmarks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edrxauqsOe4u"
      },
      "source": [
        "**1. Install the packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4L0K84qCM3iB"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install evaluate datasets accelerate\n",
        "!pip install transformers torchvision\n",
        "!pip install huggingface-hub hf_xet\n",
        "#Hold tight, this will take around 1-2 minutes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7RGZZv5vCbS"
      },
      "source": [
        "**Dataset ID2Label Mapping**\n",
        "\n",
        "Note : The `id2label` mapping shows how numerical class IDs correspond to human-readable labels.  \n",
        "This is **not required** for training or evaluation ‚Äî it's just for **visual reference** and **debugging**.\n",
        "\n",
        "    To demonstrate the fine-tuning process, we will use the CIFAR-10 dataset, which contains labeled images for image classification.\n",
        "    You can find the CIFAR-10 dataset here: [cifar10](https://huggingface.co/datasets/uoft-cs/cifar10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwOTRQvivGrj"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"uoft-cs/cifar10\")\n",
        "\n",
        "labels = dataset[\"train\"].features[\"label\"].names\n",
        "\n",
        "id2label = {str(i): label for i, label in enumerate(labels)}\n",
        "\n",
        "print(id2label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uf8nA1zcO0Jm"
      },
      "source": [
        "**2. Import modules required for data manipulation, model training, and image preprocessing.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rssbtycmdF8"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report, f1_score\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import evaluate\n",
        "from datasets import Dataset, Image, ClassLabel\n",
        "from transformers import (\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DefaultDataCollator\n",
        ")\n",
        "\n",
        "from transformers import AutoImageProcessor, AutoProcessor\n",
        "from transformers import SiglipForImageClassification, AutoModel\n",
        "from transformers.image_utils import load_image\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import (\n",
        "    CenterCrop,\n",
        "    Compose,\n",
        "    Normalize,\n",
        "    RandomRotation,\n",
        "    RandomResizedCrop,\n",
        "    RandomHorizontalFlip,\n",
        "    RandomAdjustSharpness,\n",
        "    Resize,\n",
        "    ToTensor\n",
        ")\n",
        "\n",
        "from PIL import Image, ExifTags\n",
        "from PIL import Image as PILImage\n",
        "from PIL import ImageFile\n",
        "# Enable loading truncated images\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwUwky1LzhH3"
      },
      "source": [
        "**3. Loading and Preparing the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6YHpfeMzemH"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"uoft-cs/cifar10\", split=\"train\")\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "file_names = []\n",
        "labels = []\n",
        "\n",
        "for example in dataset:\n",
        "    file_path = str(example['img'])\n",
        "    label = example['label']\n",
        "\n",
        "    file_names.append(file_path)\n",
        "    labels.append(label)\n",
        "\n",
        "print(len(file_names), len(labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZZK-vPVzxiH"
      },
      "source": [
        "**4. Creating a DataFrame and Balancing the Dataset & Working with a Subset of Labels**\n",
        "\n",
        "> Manual Label List (for Custom Naming & Mapping Consistency)\n",
        "\n",
        "We manually define the `labels_list` to:\n",
        "\n",
        "    Avoid auto-mapping issues that may arise due to inconsistent label formats in the dataset.\n",
        "\n",
        "    Support flexible naming conventions, especially when label names need to follow a specific format or order.\n",
        "\n",
        "    Ensure consistent behavior across different tools (like `ClassLabel`, Hugging Face datasets, and visualization libraries).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zH_c7k7H1-29"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame.from_dict({\"img\": file_names, \"label\": labels})\n",
        "print(df.shape)\n",
        "\n",
        "df.head()\n",
        "df['label'].unique()\n",
        "\n",
        "y = df[['label']]\n",
        "df = df.drop(['label'], axis=1)\n",
        "ros = RandomOverSampler(random_state=83)\n",
        "df, y_resampled = ros.fit_resample(df, y)\n",
        "del y\n",
        "df['label'] = y_resampled\n",
        "del y_resampled\n",
        "gc.collect()\n",
        "\n",
        "labels_subset = labels[:5]\n",
        "print(labels_subset)\n",
        "\n",
        "#labels_list = ['example_label_0', 'example_label_1'................,'example_label_n-1']\n",
        "labels_list = [\n",
        "    'airplane',\n",
        "    'automobile',\n",
        "    'bird',\n",
        "    'cat',\n",
        "    'deer',\n",
        "    'dog',\n",
        "    'frog',\n",
        "    'horse',\n",
        "    'ship',\n",
        "    'truck'\n",
        "]\n",
        "\n",
        "label2id, id2label = {}, {}\n",
        "for i, label in enumerate(labels_list):\n",
        "    label2id[label] = i\n",
        "    id2label[i] = label\n",
        "\n",
        "ClassLabels = ClassLabel(num_classes=len(labels_list), names=labels_list)\n",
        "\n",
        "print(\"Mapping of IDs to Labels:\", id2label, '\\n')\n",
        "print(\"Mapping of Labels to IDs:\", label2id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXvzq4AY15XF"
      },
      "source": [
        "**5. Mapping and Casting Labels**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIIabbmJ0TiX"
      },
      "outputs": [],
      "source": [
        "def map_label2id(example):\n",
        "    example['label'] = ClassLabels.str2int(example['label'])\n",
        "    return example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QyBBxjo2jOO"
      },
      "source": [
        "**6. Splitting the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdBEyEI71ldo"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.map(map_label2id, batched=True)\n",
        "dataset = dataset.cast_column('label', ClassLabels)\n",
        "dataset = dataset.train_test_split(test_size=0.4, shuffle=True, stratify_by_column=\"label\")\n",
        "\n",
        "train_data = dataset['train']\n",
        "test_data = dataset['test']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgMyNfLs2yNW"
      },
      "source": [
        "**7. Setting Up the Model and Processor**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcTtRIrJ3L4N"
      },
      "outputs": [],
      "source": [
        "model_str = \"facebook/metaclip-2-worldwide-s16\"\n",
        "processor = AutoImageProcessor.from_pretrained(model_str)\n",
        "\n",
        "image_mean, image_std = processor.image_mean, processor.image_std\n",
        "size = processor.size[\"height\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuqaZWIp3u8F"
      },
      "source": [
        "**8. Defining Data Transformations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIHobuid30G-"
      },
      "outputs": [],
      "source": [
        "_train_transforms = Compose([\n",
        "    Resize((size, size)),\n",
        "    RandomRotation(90),\n",
        "    RandomAdjustSharpness(2),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=image_mean, std=image_std)\n",
        "])\n",
        "\n",
        "_val_transforms = Compose([\n",
        "    Resize((size, size)),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=image_mean, std=image_std)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xt6kH0QW3-xf"
      },
      "source": [
        "**9. Applying Transformations to the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPiM8jas4FXm"
      },
      "outputs": [],
      "source": [
        "def train_transforms(examples):\n",
        "    examples['pixel_values'] = [_train_transforms(image.convert(\"RGB\")) for image in examples['img']]\n",
        "    return examples\n",
        "\n",
        "def val_transforms(examples):\n",
        "    examples['pixel_values'] = [_val_transforms(image.convert(\"RGB\")) for image in examples['img']]\n",
        "    return examples\n",
        "\n",
        "train_data.set_transform(train_transforms)\n",
        "test_data.set_transform(val_transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-woWc8qQ4SP0"
      },
      "source": [
        "**10. Creating a Data Collator**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhosTciU4X49"
      },
      "outputs": [],
      "source": [
        "def collate_fn(examples):\n",
        "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
        "    labels = torch.tensor([example['label'] for example in examples])\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ow0U04HP4amM"
      },
      "source": [
        "**11. Initializing the Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAJBpBcO4feE"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForImageClassification\n",
        "\n",
        "model = AutoModelForImageClassification.from_pretrained(model_str, num_labels=len(labels_list))\n",
        "model.config.id2label = id2label\n",
        "model.config.label2id = label2id\n",
        "\n",
        "print(model.num_parameters(only_trainable=True) / 1e6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OZV3GId4kv2"
      },
      "source": [
        "**12. Defining Metrics and the Compute Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZMQzLWl4ijD"
      },
      "outputs": [],
      "source": [
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions = eval_pred.predictions\n",
        "    label_ids = eval_pred.label_ids\n",
        "\n",
        "    predicted_labels = predictions.argmax(axis=1)\n",
        "    acc_score = accuracy.compute(predictions=predicted_labels, references=label_ids)['accuracy']\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": acc_score\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJZbgNxk7LzB"
      },
      "source": [
        "**13. Setting Up Training Arguments**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2u4d3dg6PzK"
      },
      "outputs": [],
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir=\"metaclip-2-image-classification/\",\n",
        "    logging_dir='./logs',\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=4,\n",
        "    weight_decay=0.02,\n",
        "    warmup_steps=50,\n",
        "    remove_unused_columns=False,\n",
        "    save_strategy='epoch',\n",
        "    load_best_model_at_end=True,\n",
        "    save_total_limit=4,\n",
        "    report_to=\"none\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASw82izY7w-s"
      },
      "source": [
        "**14. Initializing the Trainer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hh9OOpq67wkh"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=test_data,\n",
        "    data_collator=collate_fn,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roNYBEmA7-mp"
      },
      "source": [
        "**15. Evaluating, Training, and Predicting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSK69Nd_7-RJ"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "trainer.evaluate()\n",
        "\n",
        "outputs = trainer.predict(test_data)\n",
        "print(outputs.metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lrr8wcI8944"
      },
      "source": [
        "**16. Computing Additional Metrics and Plotting the Confusion Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nypmCSIP9Cw6"
      },
      "outputs": [],
      "source": [
        "y_true = outputs.label_ids\n",
        "y_pred = outputs.predictions.argmax(1)\n",
        "\n",
        "def plot_confusion_matrix(cm, classes, title='Confusion Matrix', cmap=plt.cm.Blues, figsize=(10, 8)):\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=90)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.0f'\n",
        "    thresh = cm.max() / 2.0\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "if len(labels_list) <= 150:\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plot_confusion_matrix(cm, labels_list, figsize=(8, 6))\n",
        "\n",
        "print()\n",
        "print(\"Classification report:\")\n",
        "print()\n",
        "print(classification_report(y_true, y_pred, target_names=labels_list, digits=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmHX19Zd9KF4"
      },
      "source": [
        "**17. Saving the Model and Uploading to Hugging Face Hub**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0F5O5A89QqA"
      },
      "outputs": [],
      "source": [
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJfH3v0am_O4"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login, HfApi\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQLQunHCmdF8"
      },
      "outputs": [],
      "source": [
        "api = HfApi()\n",
        "repo_id = f\"prithivMLmods/MetaCLIP-2-Cifar10\"\n",
        "\n",
        "api.upload_folder(\n",
        "    folder_path=\"metaclip-2-image-classification/\",\n",
        "    path_in_repo=\".\",\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"model\",\n",
        "    revision=\"main\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWsjtCcGg26I"
      },
      "source": [
        "\n",
        "This notebook provides a complete pipeline to fine-tune MetaCLIP-2 for single-label image classification tasks. By leveraging its globally scaled, multilingual pretraining recipe ‚Äî which includes contrastive language‚Äìimage learning across 300+ languages, balanced data curation, and a multilingual tokenizer ‚Äî you can achieve strong performance on diverse datasets. Whether you're working with multilingual labels, imbalanced classes, or custom naming schemes, this setup is flexible and well suited for experimentation.\n",
        "\n",
        "    You can find the fine-tuned model here: https://huggingface.co/prithivMLmods/MetaCLIP-2-Cifar10\n",
        "\n",
        "Ready to adapt for your own datasets and downstream tasks!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukvIhV4ZZkrM"
      },
      "source": [
        "**üéâ Happy Fine-Tuning!**\n",
        "\n",
        "> **Made with ü§ó x ‚ù§Ô∏è by [Prithiv Sakthi](https://www.linkedin.com/in/prithiv-sakthi/)**\n",
        "\n",
        "Feel free to fork, modify, and explore!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}